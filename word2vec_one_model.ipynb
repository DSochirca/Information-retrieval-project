{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 3614377\n",
      "Number of unique users: 65999\n",
      "Number of unique queries: 1244495\n",
      "Query frequencies range:  [1, 98554]\n",
      "Average query length: 17.424875158291457\n"
     ]
    },
    {
     "data": {
      "text/plain": "     AnonID                            Query            QueryTime  ItemRank  \\\n0       479                       family guy  2006-03-01 16:01:20       NaN   \n1       479          also sprach zarathustra  2006-03-02 14:48:55       NaN   \n2       479      family guy movie references  2006-03-03 22:37:46       1.0   \n3       479  top grossing movies of all time  2006-03-03 22:42:42       1.0   \n4       479  top grossing movies of all time  2006-03-03 22:42:42       2.0   \n..      ...                              ...                  ...       ...   \n118     479                         nip tuck  2006-05-28 00:44:58       4.0   \n119     479                nip tuck season 4  2006-05-28 00:47:05       NaN   \n120     479            nip tuck season 3 dvd  2006-05-28 00:47:48       7.0   \n121     479            nip tuck season 3 dvd  2006-05-28 00:47:48       9.0   \n122     479            nip tuck season 3 dvd  2006-05-28 00:47:48       1.0   \n\n                          ClickURL  QueryFrequency  \\\n0                              NaN             191   \n1                              NaN               1   \n2    http://www.familyguyfiles.com               1   \n3              http://movieweb.com               2   \n4              http://www.imdb.com               2   \n..                             ...             ...   \n118         http://www.niptuck.com              24   \n119                            NaN               4   \n120        http://en.wikipedia.org               3   \n121      http://www.dvdtimes.co.uk               3   \n122        http://www.amazon.co.uk               3   \n\n                                           QueryTokens  \n0                                    ['family', 'guy']  \n1                    ['also', 'sprach', 'zarathustra']  \n2             ['family', 'guy', 'movie', 'references']  \n3    ['top', 'grossing', 'movies', 'of', 'all', 'ti...  \n4    ['top', 'grossing', 'movies', 'of', 'all', 'ti...  \n..                                                 ...  \n118                                    ['nip', 'tuck']  \n119                     ['nip', 'tuck', 'season', '4']  \n120              ['nip', 'tuck', 'season', '3', 'dvd']  \n121              ['nip', 'tuck', 'season', '3', 'dvd']  \n122              ['nip', 'tuck', 'season', '3', 'dvd']  \n\n[123 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AnonID</th>\n      <th>Query</th>\n      <th>QueryTime</th>\n      <th>ItemRank</th>\n      <th>ClickURL</th>\n      <th>QueryFrequency</th>\n      <th>QueryTokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>479</td>\n      <td>family guy</td>\n      <td>2006-03-01 16:01:20</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>191</td>\n      <td>['family', 'guy']</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>479</td>\n      <td>also sprach zarathustra</td>\n      <td>2006-03-02 14:48:55</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>['also', 'sprach', 'zarathustra']</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>479</td>\n      <td>family guy movie references</td>\n      <td>2006-03-03 22:37:46</td>\n      <td>1.0</td>\n      <td>http://www.familyguyfiles.com</td>\n      <td>1</td>\n      <td>['family', 'guy', 'movie', 'references']</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>479</td>\n      <td>top grossing movies of all time</td>\n      <td>2006-03-03 22:42:42</td>\n      <td>1.0</td>\n      <td>http://movieweb.com</td>\n      <td>2</td>\n      <td>['top', 'grossing', 'movies', 'of', 'all', 'ti...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>479</td>\n      <td>top grossing movies of all time</td>\n      <td>2006-03-03 22:42:42</td>\n      <td>2.0</td>\n      <td>http://www.imdb.com</td>\n      <td>2</td>\n      <td>['top', 'grossing', 'movies', 'of', 'all', 'ti...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>479</td>\n      <td>nip tuck</td>\n      <td>2006-05-28 00:44:58</td>\n      <td>4.0</td>\n      <td>http://www.niptuck.com</td>\n      <td>24</td>\n      <td>['nip', 'tuck']</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>479</td>\n      <td>nip tuck season 4</td>\n      <td>2006-05-28 00:47:05</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>['nip', 'tuck', 'season', '4']</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>479</td>\n      <td>nip tuck season 3 dvd</td>\n      <td>2006-05-28 00:47:48</td>\n      <td>7.0</td>\n      <td>http://en.wikipedia.org</td>\n      <td>3</td>\n      <td>['nip', 'tuck', 'season', '3', 'dvd']</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>479</td>\n      <td>nip tuck season 3 dvd</td>\n      <td>2006-05-28 00:47:48</td>\n      <td>9.0</td>\n      <td>http://www.dvdtimes.co.uk</td>\n      <td>3</td>\n      <td>['nip', 'tuck', 'season', '3', 'dvd']</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>479</td>\n      <td>nip tuck season 3 dvd</td>\n      <td>2006-05-28 00:47:48</td>\n      <td>1.0</td>\n      <td>http://www.amazon.co.uk</td>\n      <td>3</td>\n      <td>['nip', 'tuck', 'season', '3', 'dvd']</td>\n    </tr>\n  </tbody>\n</table>\n<p>123 rows Ã— 7 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "# POT needs to be installed for the following code to work\n",
    "# !pip install POT\n",
    "\n",
    "df = pd.read_csv('data/user-ct-test-collection-02-with-query-frequencies-and-tokens.txt', sep='\\t')\n",
    "\n",
    "frequencies = pd.read_csv('data/query-frequencies-precomputed.txt', sep='\\t', index_col=0)\n",
    "tokens = pd.read_csv('data/query-tokens-precomputed.txt', sep='\\t', index_col=0)\n",
    "idf_scores = pd.read_csv('data/idf-scores-precomputed.txt', sep='\\t', index_col=0)\n",
    "\n",
    "# Dataset statistics\n",
    "print('Number of rows:', df.shape[0])\n",
    "print('Number of unique users:', df['AnonID'].nunique())\n",
    "print('Number of unique queries:', df['Query'].nunique())\n",
    "print('Query frequencies range:  [{}, {}]'.format(df['QueryFrequency'].min(), df['QueryFrequency'].max()))\n",
    "print('Average query length:', df['Query'].apply(len).mean())\n",
    "\n",
    "# User 479 queries:\n",
    "df[df['AnonID'] == 479]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm  # Progress bar for training\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Tokenize queries (with tqdm progress bar)\n",
    "queries = df['Query']\n",
    "query_tokens = df['QueryTokens']\n",
    "\n",
    "\n",
    "# Dummy model for vocabulary intersection, with same parameters as the Google model\n",
    "# model = Word2Vec(sentences=[word_tokenize(query) for query in query_tokens], vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model\n",
    "# google_model = KeyedVectors.load_word2vec_format('models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "google_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Continue training the Google model on AOL dataset\n",
    "google_model.build_vocab(tqdm(query_tokens, desc='Building word2vec vocabulary'), update=True) # \"update = True\" extends existing vocabulary\n",
    "\n",
    "google_model.train(tqdm(query_tokens, desc='Training word2vec model'),\n",
    "            total_examples=google_model.corpus_count,\n",
    "            epochs=30)\n",
    "\n",
    "# Save the model\n",
    "google_model.save('models/GoogleNews_on_AOL.model')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "# Query auto-completion function\n",
    "def query_completion(query, completion_list, session_queries, alpha=0.6):\n",
    "    N = len(session_queries)\n",
    "    gamma = beta = 1/(N+1)\n",
    "    omega = 0.5\n",
    "\n",
    "    # Frequency score (of each candidate query)\n",
    "    frequency_score = [frequencies.loc[a, 'Frequency'] if a in frequencies.index else 0 for a in completion_list]\n",
    "\n",
    "    # Semantic similarity score\n",
    "    # Use average similarity score on words (average of all word pairs)\n",
    "    similarity_score = []\n",
    "    for candidate_query in completion_list:  # Possible completions\n",
    "        candidate_score = 0\n",
    "\n",
    "        for session_query in session_queries:   # User session queries\n",
    "            score = 0\n",
    "\n",
    "            c_tokens = word_tokenize(candidate_query)\n",
    "            x_tokens = word_tokenize(session_query)\n",
    "\n",
    "            # All word pairs\n",
    "            for c in c_tokens:\n",
    "                for x in x_tokens:\n",
    "                    if c in google_model.key_to_index and x in google_model.key_to_index:\n",
    "                        similarity = google_model.similarity(c, x)\n",
    "                    else:\n",
    "                        similarity = 0    # word is not in the vocabulary\n",
    "\n",
    "                    # tf-idf weighting average:\n",
    "                    if c in idf_scores.index.values and x in idf_scores.index.values:\n",
    "                        # match index\n",
    "                        c_idf = idf_scores.loc[idf_scores.index == c, 'IDF'].values[0]\n",
    "                        x_idf = idf_scores.loc[idf_scores.index == x, 'IDF'].values[0]\n",
    "\n",
    "                        score += (similarity * c_idf * x_idf)\n",
    "                    else:\n",
    "                        score += similarity\n",
    "\n",
    "            # Average over all combinations of words:\n",
    "            score /= (len(c_tokens) * len(x_tokens))\n",
    "\n",
    "            candidate_score += score\n",
    "\n",
    "        similarity_score.append(candidate_score)\n",
    "\n",
    "    # Combined score\n",
    "    combined_score = [alpha * similarity_score[i] + (1 - alpha) * frequency_score[i] for i in range(len(completion_list))]\n",
    "\n",
    "    # Re-rank completion list\n",
    "    re_ranked_list = [x for _, x in sorted(zip(combined_score, completion_list), reverse=True)]\n",
    "\n",
    "    return re_ranked_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test query auto-completion function\n",
    "query = 'car'\n",
    "completion_list = df[df['Query'].str.startswith(query)]['Query'].unique()  # Queries starting with 'car'\n",
    "session_queries = df[(df['AnonID'] == 479) & (df['Query'].str.contains('car'))]['Query'].unique()  # Queries from user 479 containing 'car'\n",
    "print(session_queries)\n",
    "\n",
    "print(\"Query auto-completion for 'car':\")\n",
    "print(query_completion(query, completion_list, session_queries))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# MRR (Mean Reciprocal Rank) evaluation\n",
    "def RR(ranked_completions, ground_truth):\n",
    "    \"\"\"\n",
    "    Reciprocal Rank (RR) for one query.\n",
    "    :param ranked_completions: list of suggested completions (higher rank first)\n",
    "    :param ground_truth: the correct suggestion of the query\n",
    "    :return: the RR score\n",
    "    \"\"\"\n",
    "    for i, completion in enumerate(ranked_completions):\n",
    "        if completion == ground_truth:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "def MRR(completion_lists, ground_truths):\n",
    "    \"\"\"\n",
    "    Mean of scores for entire dataset.\n",
    "    \"\"\"\n",
    "    total_score = 0.0\n",
    "    for i, completion_list in enumerate(completion_lists):\n",
    "        total_score += RR(completion_list, ground_truths[i])\n",
    "\n",
    "    return total_score / len(completion_lists)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  2709614 queries\n",
      "Test dataset size:  262547 sessions and 904763 queries\n",
      "First 10 sessions in test dataset: \n",
      "[['-', '-', '-', '-'], ['myspace.com'], ['pet sitter in newburyport ma', 'pet sitter in newburyport ma'], ['undefined'], ['shakira lyrics'], ['ebay', 'social security'], ['glutes', 'glutes', 'glutes', 'glutes', 'glutes', 'adultfriendfinder'], ['sandals vacations'], ['www.delta.com'], ['costco']]\n",
      "Session lengths in test dataset: \n"
     ]
    },
    {
     "data": {
      "text/plain": "1      113402\n2       51078\n3       28311\n4       17455\n5       11957\n        ...  \n122         1\n138         1\n183         1\n162         1\n150         1\nLength: 141, dtype: int64"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test datasets (arrays of query sessions, sorted by time increasing)\n",
    "with open('data/train.pkl', 'rb') as f:\n",
    "    train = pkl.load(f)\n",
    "\n",
    "with open('data/test.pkl', 'rb') as f:\n",
    "    test = pkl.load(f)\n",
    "\n",
    "# Flatten train dataset\n",
    "train = [query for session in train for query in session]\n",
    "train = pd.DataFrame(train, columns=['Query'])\n",
    "\n",
    "print(\"Train dataset size: \", len(train), \"queries\")\n",
    "print(\"Test dataset size: \", len(test), \"sessions and\", len([query for session in test for query in session]), \"queries\")\n",
    "print(\"First 10 sessions in test dataset: \")\n",
    "print(test[:10])\n",
    "# Session lengths counts\n",
    "print(\"Session lengths in test dataset: \")\n",
    "pd.Series([len(session) for session in test]).value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def get_completions(sessions, prefix_length, train_df, alpha=0.6):\n",
    "    completion_lists = []\n",
    "    ground_truths = []\n",
    "\n",
    "    for session in tqdm(sessions, desc='Generating completions'):\n",
    "        # Last query in the session is the query we want to complete\n",
    "        query_ground_truth = session[-1]\n",
    "        query_tokenized = word_tokenize(query_ground_truth)\n",
    "\n",
    "        # Skip if the query is shorter than the prefix length\n",
    "        if len(query_tokenized) < prefix_length:\n",
    "            continue\n",
    "\n",
    "        # Queries preceding the last query in the session\n",
    "        previous_queries = session[:-1] if len(session) > 1 else []\n",
    "\n",
    "        # Get prefix (part of the query that needs to be completed)\n",
    "        query_prefix = ' '.join(query_tokenized[:prefix_length])\n",
    "\n",
    "        # Get completions\n",
    "        completions = train_df[train_df['Query'].str.startswith(query_prefix + \" \")]['Query'].unique() # here we add a space after the word so we don't look for partial matches\n",
    "\n",
    "        # Re-rank completions\n",
    "        ranked_completions = query_completion(query_prefix, completions, previous_queries, alpha)\n",
    "\n",
    "        completion_lists.append(ranked_completions)\n",
    "        ground_truths.append(query_ground_truth)\n",
    "\n",
    "    return completion_lists, ground_truths\n",
    "\n",
    "def evaluate_test_set(test, prefix_length, alpha=0.6):\n",
    "    completion_lists, ground_truths = get_completions(test, prefix_length, train, alpha)\n",
    "    mrr = MRR(completion_lists, ground_truths)\n",
    "    return mrr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the test set\n",
    "prefix_length = 4  # query will be completed based on the first 3 words\n",
    "mrr = evaluate_test_set(test, prefix_length)\n",
    "print(\"MRR for prefix length\", prefix_length, \":\", mrr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the test set\n",
    "prefix_length = 3  # query will be completed based on the first 3 words\n",
    "mrr = evaluate_test_set(test, prefix_length)\n",
    "print(\"MRR for prefix length\", prefix_length, \":\", mrr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the test set\n",
    "prefix_length = 2  # query will be completed based on the first 2 words\n",
    "mrr = evaluate_test_set(test, prefix_length)\n",
    "print(\"MRR for prefix length\", prefix_length, \":\", mrr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the test set\n",
    "prefix_length = 1  # query will be completed based on the first word (VERY SLOW)\n",
    "mrr = evaluate_test_set(test, prefix_length)\n",
    "print(\"MRR for prefix length\", prefix_length, \":\", mrr)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
